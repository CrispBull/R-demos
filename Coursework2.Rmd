---
title: "Blood Pressure and Dietary Calcium"
author: "Chidiebere Nwokocha"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Dependencies
library(knitr)
library(kableExtra)
library(gridExtra)
library(tidyverse)
library(xtable)
library(readxl)
library(writexl)
library(hrbrthemes)
library(cowplot)
library(viridis)

## References
# https://www.cpc.unc.edu/projects/china
```

### Introduction

```{r, echo=FALSE, cache=TRUE}
description <- tibble(
  id = "Participants identification number each survey year. This is a yearly survey that follows participants across many years and the ids are unique to these participants and reused for the given participant each year", 
  age = "Participant age at each survey year",
  location = "Participants living location at the time of survey. This categories here can be 1 or 2 where 1 denotes urban location and 2 for rural location",
  gender = "Participants gender. 1 for male and 2 for female",
  nation = "Participant nationality",
  waves = "The survey year. The dataset starts from 2000 to 2009 and within this period the survey has held for 2000, 2004, 2006 and 2009",
  smoking = "Denotes if the given participant smokes or not. 0 for non-smokers and 1 for smokers",
  alcohol = "This is the alcohol consumption frequency per week. 1 unit represents one glass of alcohol. The values here tell how many units participant took per week on average",
  dgHBP = "This is the hypertension diagnostic report. Categories here are either 0 or 1. 0 meaning no hypertention diagnostic report and 1 for any diagnostic report",
  sbp = "This is the average (mean) systolic blood pressure measurement taken from 3 measurements",
  dbp = "This is the average (mean) diastolic blood pressure measurement taken from 3 measurements",
  bmi = "The Body ass Index calculated using participan's weight and height with an Adolphe Quetelet Formula (Weight (Cm)/[Height (m)]^2)",
  nrg = "Energy intake of the participant expressed in kCal",
  dietCa = "Partiicpants dietary calcium intake measured in mg",
  met_m = "Physical activity of participants expressed in the number of hours per week of physical activity"
  )

description_table <- pivot_longer(
  description,
  cols = everything(),
  names_to = "Variable",
  values_to = "Description"
)


kable(description_table, caption = "Table 1: Dataset variables and description") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"))
```

```{r, echo=FALSE, cache=TRUE}
## Importing data used in analysis. 

bp_raw_data <- read_xlsx("~/Desktop/coursework2 BI datasets/DATASET_DIETARY CALCIUM INTAKE AND HYPERTENSION IN URBAN AND RURAL POPULATIONS.xlsx")
bp_calcium_data <- bp_raw_data
```

### Analysis of raw data

After importing our data to our workspace, it looks like **Table 1** below;

```{r, echo=FALSE, cache=TRUE}
data_types <- sapply(bp_raw_data, class)
summary_types <- data.frame(
  "Data type" = data_types
)

kable(summary_types, caption = "Table 2: Variables data types", booktabs = TRUE) %>%
  kable_classic(c("striped"), full_width = T)
```

From the above we can see that our raw data is imported as numerical values for all columns even when we know some are categorical values and even a date type column (Waves) based on the column metadata in **Table 1**. We can take a glimpse of how the data looks below;

```{r, echo=FALSE, cache=TRUE}
no_of_rows <- nrow(bp_calcium_data)
no_of_cols <- ncol(bp_calcium_data)


bp_level <- function(systolic, diastolic) {
  case_when(
    systolic < 120.0 & diastolic < 80.0 ~ "Normal",
    (systolic >= 120.0 & systolic < 130) & diastolic < 80.0 ~ "Elevated",
    (systolic >= 130.0 & systolic < 140.0) | (diastolic >= 80.0 & diastolic < 90.0) ~ "HBP Stage 1", ## considered normal
    (systolic >= 140.0 & systolic < 180.0) | (diastolic >= 90.0 & diastolic < 120.0) ~ "HBP Stage 2",
    systolic >= 180.0 | diastolic >= 120.0 ~ "HBP Stage 3",
    TRUE ~ "Uncategorized"
  )
}

#gender_counts <- table(bp_calcium_data$Gender)
#gender_counts_prop <- prop.table(gender_counts)

#bp_females <- bp_calcium_data |> 
  #filter(Gender == 2)
#bp_males <- bp_calcium_data |>
#  filter(Gender == 1)


minAge <- min(bp_calcium_data$Age)
maxAge <- max(bp_calcium_data$Age)
avgAge <- mean(bp_calcium_data$Age)

avgBmi <- mean(bp_calcium_data$BMI, na.rm = TRUE)
bmi_na_count <- sum(is.na(
  bp_calcium_data$BMI
))

prop_bmi_missing <- (bmi_na_count/no_of_rows) * 100

raw_random <- bp_raw_data |>
  filter(is.na(SBP) | is.na(DBP) | is.na(BMI)) |>
  head()

kable(raw_random, caption = "Table 3: Sample from data", booktabs = TRUE) %>%
  kable_classic(c("striped"), full_width = T)
```

From the above we can see several numerical variables as well as notice that there are missing values (NA) in our dataset. The dataset contains `r no_of_rows` rows and `r no_of_cols` columns. The minimum age of a participant in the dataset is `r minAge` and the maximum age is `r maxAge`. The average age participants around `r round(avgAge)` years.

The average BMI of participants is around `r round(avgBmi, 2)`, which suggests a healthy population. However there over `r bmi_na_count` missing values, which makes up about `r round(prop_bmi_missing, 1)`% of data. The following table provides a summary of the BMI;

```{r, echo=FALSE, cache=TRUE}
bmi_summary <- bp_calcium_data |>
  select(BMI) |>
  #filter(!is.na(SBP)) |>
  summarise(
    "Minimum" = round(min(BMI, na.rm = TRUE), 2),
    "Maximum" = round(max(BMI, na.rm = TRUE), 2),
    "Mean" = round(mean(BMI, na.rm = TRUE), 2),
    "Median" = round(median(BMI, na.rm = TRUE), 2),
    "Missing values" = as.integer(sum(is.na(BMI)))
  ) |>
  pivot_longer(
    cols = everything(),
    names_to = "Summary",
    values_to = "Value"
    )

kable(bmi_summary, caption = "Table 2: BMI summary (cm/m^2)", booktabs = TRUE) %>%
  kable_material(c("striped", "hover"), full_width = T)

```

From the above we can see that the minimum BMI in the data set is around 13, which is an unhealthy BMI and falls within a severely underweight BMI, the maximum BMI is also a very unhealthy number, which both calls for more inquiry into those values. We also notice that the mean and median are very close, which means the data is evenly distributed. The charts below provide some visual clarity;

```{r echo=FALSE, cache=TRUE, warning=FALSE, out.width="100%"}
## na values automatically removed
bmi_with_na_plot <- bp_calcium_data |>
  filter(!is.na(BMI)) |>
  ggplot(aes(x = BMI, y = ..density..)) +
  geom_histogram(binwidth = .5, fill = "#69b3a2", color = "#e9ecef", alpha = 0.6) + #aes(y = ..density..), 
  geom_density(color = "#69b3a2", alpha = 0.8) +
  labs(
    title = "BMI distribution",
    caption = "Fig1"
    ) +
  geom_vline(aes(xintercept = avgBmi), color = "black", linetype = "dashed", size = 1) +
  theme_ipsum() +
  theme(
    plot.title = element_text(size = 12)
  )

bmi_boxplot <- bp_calcium_data |> 
  filter(!is.na(BMI)) |>
  ggplot(aes(x = factor(Location), y = BMI, fill = factor(Location))) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha = 0.9) +
  geom_jitter(color = "grey", size = 0.2, alpha = 0.4) +
  theme_ipsum() +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 12)
  ) +
  xlab("Location") +
  labs(
    title = "BMI by Location",
    subtitle = "Boxplot with jittering to show density",
    caption = "Fig2"
    ) 
plot_grid(bmi_with_na_plot, bmi_boxplot, ncol = 2)

location_prop <- prop.table(table(factor(bp_calcium_data$Location)))
```

In **Fig1** above, we can see that the BMI is evenly distributed but there are outliers which we can clearly see in **Fig2**. In both charts we can see that data the mean and median are both concentrated around 23 for the BMI, suggesting a healthy population sample. From **Fig2** we can also see that there are more data collected for Location 2, and in fact, about `r round(location_prop[2], 2) * 100`% of participants are from rural areas (Location 2). We also see that the average BMI of urban participants (Location 1) is slightly higher than that of rural participants.

For the blood pressure measurements (systolic and diastolic measures), we notice a similar pattern in the distribution of the table, **Table 3** and **Table 4** gives an general summary of both variables.

```{r, echo=FALSE, cache=TRUE, warning=FALSE}
### systolic and diastolic
systolic_na_prop <- round((mean(is.na(bp_calcium_data$SBP)) * 100), 1)
diastolic_na_prop <- round((mean(is.na(bp_calcium_data$DBP)) * 100), 1)

sbp_summary <- bp_calcium_data |>
  select(SBP) |>
  #filter(!is.na(SBP)) |>
  summarise(
    "Minimum" = round(min(SBP, na.rm = TRUE), 2),
    "Maximum" = round(max(SBP, na.rm = TRUE), 2),
    "Mean" = round(mean(SBP, na.rm = TRUE), 2),
    "Median" = round(median(SBP, na.rm = TRUE), 2),
    "Missing values" = as.integer(sum(is.na(SBP)))
  ) |>
  pivot_longer(
    cols = everything(),
    names_to = "Summary",
    values_to = "Value"
    )

dbp_summary <- bp_calcium_data |>
  select(DBP) |>
  summarise(
    "Minimum" = round(min(DBP, na.rm = TRUE), 2),
    "Maximum" = round(max(DBP, na.rm = TRUE), 2),
    "Mean" = round(mean(DBP, na.rm = TRUE), 2),
    "Median" = round(median(DBP, na.rm = TRUE), 2),
    "Missing values" = as.integer(sum(is.na(DBP)))) |>
  pivot_longer(
    cols = everything(),
    names_to = "Summary",
    values_to = "Value"
    )

kable(sbp_summary, caption = "Table 4: Systolic Blood pressure summary (mmHg)") %>%
  #kable_classic_2(full_width = F, position = "float_right") %>%
  #kable_material(c("striped", "hover"), full_width = F, position = "float_left") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "condensed"), position = "float_right") %>%
  column_spec(2, width = "25em") 

kable(dbp_summary, caption = "Table 3: Diastolic Blood pressure summary (mmHg)") %>%
  #kable_classic_2(full_width = F, position = "float_right") %>%
  #kable_material(c("striped", "hover"), full_width = F, position = "float_left") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "condensed"), position = "left") %>%
  column_spec(2, width = "20em")

```

In **Table 3**, we see a summary of the Diastolic measurements of the data. We observe that the values range from 25*mmHg* to 170*mmHg*, the mean and median are also around the same value which indicates that the distribution is approximately symmetrical (not skewed) and data is evenly distributed around the center (bell curve). We also observe that there are 854 missing values, which makes up about `r diastolic_na_prop`% of the data, a significant proportion. **Table 4** shows a summary of the Systolic Blood pressure measurements. We can see that the the minimum and maximum values ranges from 76.67**mmHg** to 240**mmHg**, the difference in the mean and median values also suggests that the distribution is skewed unlike the diastolic measures. Also, about 7% of the values are missing.

```{r, echo=FALSE, cache=TRUE, warning=FALSE, out.width="80%"}
systolic_diastolic_reshaped <- bp_calcium_data |>
  select(SBP, DBP, Location) |>
  pivot_longer(cols = c(SBP, DBP), names_to = "BP Measure", values_to = "Value") |>
  filter(!is.na(Value))
  
bp_density_plot <- systolic_diastolic_reshaped |>
  ggplot(aes(x = Value, fill = `BP Measure`, color = `BP Measure`)) + 
  geom_density(alpha = 0.6) +
  theme_ipsum() +
  xlab("Systolic and Diastolic blood pressure") +
  ylab("") +
  scale_fill_discrete(
    name = "Blood Pressure Measure",
    labels = c("Diastolic", "Systolic")
    ) +
  scale_color_discrete(
    name = "Blood Pressure Measure",
    labels = c("Diastolic", "Systolic")
  ) +
  labs(caption = "Fig3") 

bp_density_plot
```

In the **Fig3** above, we can see that both distributions are look evenly distributed, however the systolic plot a long tail on the right, suggesting that there are some outliers. We also see that most of the systolic variables falls between 100 and 140. We can get a view of the distribution and spread when we look at the violin chart below.

```{r, echo=FALSE, cache=TRUE, warning=FALSE, out.width="80%"}
systolic_diastolic_violin <- systolic_diastolic_reshaped |>
  ggplot(aes(x = `BP Measure`, y = Value, fill = `BP Measure`)) +
  geom_violin(alpha = 0.7) +
  #facet_wrap( ~ factor(Location)) +
  theme_bw() +
  xlab("Diastolic / Systolic Blood Pressure") +
  ylab("Value") +
  labs(caption = "Fig4") +
  theme(legend.position="none") 

systolic_diastolic_violin
```

In **Fig4** above, we can clearly see the range of spread in the measures, we also see that the systolic measure is skewed because of the unevenness of it's tails, unlike the diastolic measure which is more fairly distributed. Therefore we might need to look at the outliers here as well before analysis. We can also take a look at the distribution across locations as shown in **Fig5** below.

```{r, echo=FALSE, cache=TRUE, warning=FALSE, out.width="80%"}
bp_violin_locations <- systolic_diastolic_reshaped |>
  ggplot(aes(x = `BP Measure`, y = Value, fill = `BP Measure`)) +
  geom_violin(alpha = 0.7) +
  facet_wrap( ~ factor(Location)) +
  theme_bw() +
  xlab("Diastolic / Systolic Blood Pressure") +
  ylab("Value") +
  labs(caption = "Fig5") +
  theme(legend.position="none") 

bp_violin_locations
```

As we can see from the above chart, we can see that the distribution is very similar across locations, so a very good sample. We can also see that the measures of Location 2 is more widely spread than Location 1.

Dietary calcium is core to our analysis and looking at the data, we can see that participants take a very varying amount of dietary calcium. The following table shows a summary of the dietary calcium intake by participants

```{r, echo=FALSE, cache=TRUE}

dCal_summary <- bp_calcium_data |>
  select(DietCa) |>
  summarise(
    "Minimum" = round(min(DietCa, na.rm = TRUE), 2),
    "Maximum" = round(max(DietCa, na.rm = TRUE), 2),
    "Mean" = round(mean(DietCa, na.rm = TRUE), 2),
    "Median" = round(median(DietCa, na.rm = TRUE), 2),
    "Missing values" = round(sum(is.na(DietCa)), 0)
  ) |>
  pivot_longer(
    cols = everything(),
    names_to = "Summary",
    values_to = "Value"
    )

kable(dCal_summary, caption = "Table 5: Dietary calcium summary", booktabs = TRUE) %>%
  kable_material(c("striped", "hover"), full_width = T)
```

From **Table 5** above, we can see that the mimimum dietary calcium intake is 0*mg* and the maximum is 19,671*mg*, these are both way outside the recommended proportions of calcium per day which we would explore in detail for these values. We can also see the difference between the mean and median values which shows that the distribution is skewed as the following chart shows.

```{r, echo=FALSE, cache=TRUE}
calc_intake_density <- bp_calcium_data |>
  select(Location, DietCa) |>
  ggplot(aes(x = DietCa)) +
  geom_histogram(binwidth = 30, fill = "#69b3a2") +
  geom_density(color = "#69b3a2", fill = "#69b3a2") +
  scale_x_continuous(breaks = seq(0, 20000, 2000)) +
  theme_ipsum() +
  labs(
    caption = "Fig6"
  ) +
  xlab("Calcium intake/day (mg)") +
  ylab("") 

calc_intake_density
```

In **Fig6** we can see that the data is massively skewed, we also observe that majority of participants take between below 500mg per day, therefore something might be wrong with the data to cause such a massive skew. We can properly visualize the place where the data is coming from with a boxplot/violin plot and see the density of where a bulk of the values lies.

```{r, echo=FALSE, cache=TRUE}
diet_calc_box <- bp_calcium_data |> 
  select(Location, DietCa) |>
  ggplot(aes(x = factor(Location), y = DietCa, fill = factor(Location))) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha = 0.9) +
  geom_jitter(color = "grey", size = 0.2, alpha = 0.4) +
  scale_y_continuous(breaks = seq(0, 20000, 1500)) +
  theme_ipsum() +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 12)
  ) +
  xlab("Location") +
  ylab("Calcium intake/day (mg)") +
  labs(
    title = "Dietary Calcium by Location",
    caption = "Fig7"
    ) 

diet_calc_box
```

From **Fig7** above, we can see that the outliers drastically skewed data, especially the data from participants in Location 2, so in order to work with this measure, we need to address this.

```{r, echo=FALSE, cache=TRUE, warning=FALSE, out.width="80%"}

# Gender
gender_1_prop <- round((mean(bp_calcium_data$Gender == 1) * 100), 2)
gender_2_prop <- round((mean(bp_calcium_data$Gender == 2) * 100), 2)

gender_factors <- factor(bp_calcium_data$Gender)
bp_calcium_data$Gender <- gender_factors

gender_barchart <- bp_calcium_data |>
  select(Gender) |>
  ggplot(aes(x = Gender, fill = Gender)) +
  geom_bar(alpha = 0.7, width = 0.4) +
  theme_bw() +
  xlab("Gender") +
  theme(
    legend.position = "none"
  )
  
  # scale_fill_discrete(
  #   name = "Gender",
  #   labels = c("Male", "Female")
  #   ) +
  # scale_color_discrete(
  #   name = "Gender",
  #   labels = c("Male", "Female")
  # ) 

## Smoking
smoke_factors <- factor(bp_calcium_data$Smoking)
bp_calcium_data$Smoking <- smoke_factors
#table(bp_calcium_data$Smoking, useNA = "always")
smoker_formatted <- bp_calcium_data |>
  filter(!is.na(Smoking)) |>
  mutate(Smoke = fct_recode(Smoking,
                           "Non-smoker" = "0",
                           "Smoker" = "1"
                           )
         )


smoker_barchart <-  smoker_formatted |>
  select(Smoke) |>
  ggplot(aes(x = Smoke, fill = Smoke)) +
  geom_bar(alpha = 0.7, width = 0.4) +
  theme_bw() +
  xlab("Smoke") +
  theme(
    legend.position = "none"
  ) 

## Diagnostic Report
diagnostic_factors <- factor(bp_calcium_data$DgHBP)
bp_calcium_data$DgHBP <- diagnostic_factors
#table(bp_calcium_data$DgHBP, useNA = "always") # no missing values

diagnosed_formatted <- bp_calcium_data |>
  mutate(Diagnosed = fct_recode(DgHBP,
                           "No report" = "0",
                           "Has report" = "1"
                           )
         )


diagnosis_barchart <-  diagnosed_formatted |>
  select(Diagnosed) |>
  ggplot(aes(x = Diagnosed, fill = Diagnosed)) +
  geom_bar(alpha = 0.7, width = 0.4) +
  theme_bw() +
  xlab("Hypertention Diagnosis") +
  theme(
    legend.position = "none"
  )

## Alcohol Consumption Frequency
alcohol_consumption_plot <- bp_calcium_data |>
  filter(!is.na(Alcohol)) |>
  ggplot(aes(y = Alcohol)) +
  geom_boxplot(fill = "#69b3a2", alpha = 0.7) +
  scale_y_continuous(breaks = seq(0, 8, 2)) +
  theme_bw() +
  ylab("Glass per week") +
  xlab("Alcohol consumption per week") +
  theme(
    plot.title = element_text(size = 10)
  )

plot_grid(
  gender_barchart, smoker_barchart, diagnosis_barchart, alcohol_consumption_plot, ncol = 2, labels = "auto"
  )
```

In the dataset, both genders are represented although which gender it is isn't specified. We can observe that Gender 1 makes up `r gender_1_prop`% of the data and gender 2 makes up the remaining `r gender_2_prop`%. This is represented in plot **a** above as we can see from the barchart. We can also see that most of the participants are non smokers (plot **b**). In plot **c** we can see that most of the participants are not reported to be hypertensive after diagnosis, so hypertension isn't prevalent amongst participants. In plot **d** we can see that most of the participants drink at least 0 glass of alcohol per week although there are participants who average 3+ glasses per week as seen in the boxplot. Also, there are missing data for Alcohol(59), Smoking(2).

### Data Cleaning and Exploratory Data Analysis.

In the above section, we went through most of the variables in our raw dataset to understand the distribution of the data as well as other necessary details like missing data, outliers, and other necessary summary statistic which are necessary to help us prepare our data for more indepth analysis which we would perform in this section.

#### Cleaning and preparing data

In order to make sure we properly analyse data and gain insights that are not flawed or biased, we need to clean our dataset by handling issues like missing values, data entry mistakes (which might lead to outliers or missing values) and skew results, duplicates, etc. This way we can ensure our data is valid and insights are meaningful. To properly prepare it for analysis, we might also go further to make sure that the variables are in the correct data type, and sometimes even create new variables or even rename columns and values for better clarity or insights.

In **Table 1** we saw that several categorical types are imported as numerical (continuous) variables, these includes the variables *Location*, *Nation*, *Smoking*, which we would need to convert to categorical values (factor) and maybe give more descriptive names like renaming 1 in the Location variable to Urban and 2 to Rural. We can also see that *Waves*, which represents the survey years is imported as numeric so we would change it to a date column, but for the purpose of our analysis that may not be necessary. We can also rename some columns to make it clear what they do, like renaming **id** to **ParticipantId**, because at its current naming it seems like value is unique but it isn't really unique as it mostly represents the id of a participant so the same id occurs multiple times for each survey year, we also change the name to be uppercamelcase to maintain consistency with other variable names.

```{r, echo=FALSE, cache=TRUE}
bp_calcium_data <- bp_raw_data |>
  rename(ParticipantId = id, Year = Waves) |>
  mutate(
    Location = fct_recode(
      as.factor(Location),
      "Urban" = "1",
      "Rural" = "2"
      ),
    Year = as.factor(Year),
    Gender = as.factor(Gender),

    Nation = as.factor(Nation),
    Smoking = fct_recode(
      as.factor(Smoking),
      "Non Smoker" = "0",
      "Smoker" = "1"
      ),
    DgHBP = fct_recode(
      as.factor(DgHBP),
      "Not Hypertensive" = "0",
      "Hypertensive" = "1"
      )
  )

data_types <- sapply(bp_calcium_data, class)
summary_types <- data.frame(
  "Data type" = data_types
)

kable(summary_types, caption = "Table 6: Variables data types", booktabs = TRUE) %>%
  kable_classic(c("striped"), full_width = T)

```

After doing these above, our table columns now looks like the table above, compared to **Table 1** we saw earlier. Also, if we take a glimpse of our dataset we see something much clearer like the table below;

```{r, echo=FALSE, echo=FALSE, cache=TRUE}
cleaned_random <- bp_calcium_data |>
  filter(is.na(SBP) | is.na(DBP) | is.na(BMI)) |>
  head()

kable(cleaned_random, caption = "Table 7: Sample from data", booktabs = TRUE) %>%
  kable_classic(c("striped"), full_width = T)
```

From **Table 7** above we can see clearer names for categorical values. However we can also see that there are missing values as well as other issues we found earlier when we looked at the columns.

From initial analysis, we saw that the following variables had missing values; *Smoking*, *Alcohol*, *SBP*, *DBP*, *BMI*, as well as *Met_m*. While there are many ways of dealing with missing data, it is largely dependent on the extent of missing data and context or even domain knowledge around the variable. Some strategies for dealing with missing data could include deletion, imputation or even model based methods. In our situation, we would be making use of both deletion and imputation methods.

```{r, echo=FALSE, cache=TRUE}

# Alcohol
missing_alcohol <- bp_calcium_data |>
  filter(is.na(Alcohol)) |>
  select(ParticipantId)

missing_alcohol_vector = missing_alcohol$ParticipantId

alcohol_missing <- bp_calcium_data  |>
  select(ParticipantId, Alcohol) |>
  filter(!is.na(Alcohol)) |>
  group_by(ParticipantId) |>
  summarise(
    Alcohol = mean(Alcohol)
  ) |>
  filter(ParticipantId %in% missing_alcohol_vector)

# DBP
dbp_avg <- bp_calcium_data |>
  select(DBP, Location) |>
  group_by(Location) |>
  summarise(DBP = mean(DBP, na.rm = TRUE))

# SBP 
sbp_imput <- bp_calcium_data |>
  select(SBP, Location, Gender) |>
  group_by(Location, Gender) |>
  summarise(SBP = mean(SBP, na.rm = TRUE))

# BMI
bmi_avg <- bp_calcium_data |>
  select(BMI, Location) |>
  group_by(Location) |>
  summarise(BMI = mean(BMI, na.rm = TRUE))


cleaned_bp_data <- bp_calcium_data |>
  filter(!is.na(Smoking)) |> # Smoking
  left_join(alcohol_missing, by = "ParticipantId", suffix = c("", "_from_missing")) |> # imputed data for Alcohol
  mutate(
    Alcohol = coalesce(Alcohol, Alcohol_from_missing)
  ) |>
  select(-Alcohol_from_missing) |> 
  left_join(dbp_avg, by = "Location", suffix = c("", "_from_missing")) |> # imputed data for DBP
  mutate(
    DBP = coalesce(DBP, DBP_from_missing)
  ) |>
  select(-DBP_from_missing) |>
  left_join(sbp_imput, by = c("Location", "Gender"), suffix = c("", "_from_missing")) |> # imputed data for SBP
  mutate(
    SBP = coalesce(SBP, SBP_from_missing)
  ) |>
  select(-SBP_from_missing) |>
  left_join(bmi_avg, by = "Location", suffix = c("", "_from_missing")) |> # imputed data for BMI
  mutate(
    BMI = coalesce(BMI, BMI_from_missing)
  ) |>
  select(-BMI_from_missing) |>
  filter(!is.na(Met_m))
```

In our dataset, we can see that only two items are missing in the Smoking column, both participants (ParticipantId 1861 and ParticipantId 1933) have a diagnosis report of not being hypertensive and have values that fall within the normal distribution for the systolic and diastolic blood pressure measurement, so it seems like these samples are already well represented and it wouldn't affect our result to remove these rows from our dataset.

For Alcohol, there are 59 missing values, a small amount, however we would be inputing the data. Our method for inputing would be based on the participant id and their average number of glasses in rows where they have data for Alcohol. So for example, for participantId 1145 which has 2 missing glasses of alcohol values, we would get the average (mean) number of glasses of alcohol this participant takes and the use that to input the glasses of alcohol for the missing value, and we would do this for each of the distinct ParticipantId.

From our initial analysis, we saw that the Diastolic measure (DSB column) is evenly distributed and fairly symmetrical with both mean and median approximately 80**mmHg**, so given the distribution, the mean is robust enough as it properly generalizes the data we have. We would also apply the missing data based on the location by getting the average measure for the location of the participant. For the Systolic measures (SBP) we would do something similar but instead of using the mean, we would use the median, the reason for this is because based on our earlier analysis, we noticed that there are variables which skewed the distribution and median is more robust than mean for such distributions, so it better captures the distribution. In the case of SBP, we also didn't just group by Location, we also grouped by gender to get the median values used for impute.

For BMI missing data, we would use the mean to fill the missing data because we can see from the analysis earlier that the distribution is distributed and slightly vary by location. And for the Met_m variable, we would simply remove rows with missing data as there are just 8 of them.

While there are many more ways to handle missing data which could get complex based on result of contextual analysis, our methods have been effective as it ensured our distribution didn't change shape.

We also explored the dietary calcium column earlier in **Fig6** and **Fig7**, and we noticed that the data was massively skewed due to some outliers that seems like impossible numbers. According to the US National Institute of Health, the recommended daily intake varies by age as shown in the following table;

```{r, echo=FALSE, warning=FALSE}
calcium_recommended <- tibble(
  "0-6 months" = "200 mg", 
  "7-12 months" = "260 mg",
  "1-3 years" = "700 mg",
  "4-8 years" = "1,000 mg",
  "9-13 years" = "1,300 mg",
  "14-18 years" = "1300 mg",
  "19-50 years" = "1,000 mg",
  "Men 51-70 years" = "1,000 mg",
  "Women 51-70 years" = "1,200 mg"
  )

description_table <- pivot_longer(
  calcium_recommended,
  cols = everything(),
  names_to = "Age",
  values_to = "Recomendation"
)

#quantile(cleaned_bp_data$DietCa, probs = seq(0.1, 1, by = 0.01))

kable(description_table, caption = "Recommended Calcium daily intake") %>%
  kable_classic(bootstrap_options = c("striped", "condensed"))
```

From the above table, if we compare with *Fig7* we would see that several of the participants take above 1,500*mg/day*. For the data point causing the most skew, if we filter our data we would see that this participant (participantId 2898) had that value in 2000, meanwhile his values in other years are all below 500 even when other variables seem pretty constant, so there's a high chance this was an input error and so we would be removing this outlier. Another reason for removing this outlier is that it offers nothing of importance to our analysis as there's too few of them around those values. Concerning the other values in this column that are shown as outliers in *Fig7*, if we check the quantiles we would see that 99% of participants take below 1500*mg* and only about 1% take above this. For the purpose of our analysis comparing Blood pressure, we would leave these values as features as it doesn't like they were input errors.

We can go further to perform other actions like reshaping the data, create new variables, etc. But these are mostly dependent on context and intent of analysis or question being asked when done at this stage. After doing the cleanup above, our dataset is now looking cleaner with no missing values, values properly imputed or removed as well as outliers addressed. Our data now looks like this compared to **Table 3**

```{r, echo=FALSE, cache=TRUE}
cleaned_bp_data <- cleaned_bp_data |>
  filter(DietCa < 15000)

data_head <- cleaned_bp_data |>
  filter(
    (ParticipantId == 6 & Year %in% c(2009, 2006)) |
      (ParticipantId == 19 & Year %in% c(2009, 2006, 2000)) |
      (ParticipantId == 35 & Year %in% c(2009))) 

  
kable(data_head, caption = "Table 3: Sample from data", booktabs = TRUE) %>%
  kable_classic(c("striped"), full_width = T)
```

In the above table taking at the same slice from **Table 3** we can see that the data is looking different with no missing values as well. Up next we would start analysing our cleaned data to get more indept insights and identify patterns.

#### Exploratory Data Analysis.
